---
title: "Assignment 4"
author: "Nzambuli Daniel 665721"
date: "2024-03-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question

Canonical Correlation Analysis (CCA) is a statistical technique used to explore the relationships between two sets of variables. Given two multivariate datasets, X and Y, CCA aims to identify linear combinations of variables from each dataset that exhibit the highest correlation. However, when dealing with high-dimensional data where the number of variables in X and/or Y is very large, traditional CCA methods require modifications to handle the increased complexity. The paper authored by Gonzalez et al. (2008) introduces a regularized CCA approach (outlined in section 2.4) specifically designed for high-dimensional datasets. While this method is suggested, alternatives from existing literature may also be explored. This homework assignment focuses on implementing both classical CCA and its high-dimensional counterpart.

> -   The task involves applying the chosen approach to the nutrimouse dataset, accessible through the CCA R package. Detailed information regarding the dataset can be found in the corresponding paper.
>
> -   For visualization, a two-dimensional graph is generated by examining the first two dimensions resulting from CCA.
>
> -   The assignment is to be completed individually, with the submission of a report written in RMarkdown format. The report should encompass: ▪ A brief overview of both classical and high-dimensional CCA methodologies implemented. ▪ Application of the preferred high-dimensional CCA method to the nutrimouse data set. ▪ Classical CCA performed on multivariate data with p \< n. (Note: Due to limitations, classical CCA cannot be applied directly to the entire X data matrix. Data should be subsetted to reflect the scenario of p \< n.) ▪ High-dimensional CCA conducted on data with p \> n. ▪ Interpretation and conclusions drawn from the data analysis results

# Working

## The Data

```{r}
library(CCA)
data(nutrimouse)
head(nutrimouse)
```

## Data Explanation

1.  `Gene` **120** genes involved in nutrition problems
2.  `Lipid` concentrations of **21** hepatic fatty acids
3.  `Genotype` **2 factors** wild-type and PPARalpha
4.  `Diet` **5factors** oils used for experimental diets preparation

**Diets**

-   corn and colza oil (50/50) – *reference diet `REF`*

-   hydrogeneated coconut oil – *saturated fatty acid `COC`*

-   sunflower oil – *Omega6 fatty acid-rich diet `SUN`*

-   linseed oil – *Omega3-rich diet* `LIN`

-   Corn/ colza/ enriched fish oil – *Fish diet* `43/43/14`

## Selecting X and Y

```{r}
X = nutrimouse$gene
Y = nutrimouse$lipid

print("X")
head(X)

print("and Y")
head(Y)
```

the gene has different dimensions from the lipid data

```{r}
cat("The dimensions of Genetic Data are", dim(X), "\n")
cat("The dimensions of Lipid Data are", dim(Y), "\n")

```

# Canonical Correlation

A statistical technique to understand the relationship between two variables by finding a linear combination of the variables that gives the most correlation between the variables

1.  Find the `weighted value` of the data to know the importance of the data point in correlation
2.  Find the `combination of values` that give the maximum correlation and repeat this finding linearly independent combinations of the features(lipid and gene) that explain the correlation of the mouse data
3.  Identify the relationship illustrated by each of the canonical covariates

Canonical correlation finds how data sets are related and how well they predict each other

# Classical vs High Dimensional CCA

+--------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+
| Feature            | Classical                                                                                                | High-Dimensional                                                                                     |
+====================+==========================================================================================================+======================================================================================================+
| **Dimensionality** | Number of observations is greater than number of variables                                               | Number of variables is larger than the number of observations                                        |
|                    |                                                                                                          |                                                                                                      |
|                    | $$                                                                                                       | $$                                                                                                   |
|                    | n > p                                                                                                    | p > n                                                                                                |
|                    | $$                                                                                                       | $$                                                                                                   |
|                    |                                                                                                          |                                                                                                      |
|                    | variables are less than the samples                                                                      | variables are more than the samples                                                                  |
+--------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+
| **Computation**    | uses a `covariance matrix` and eigen vectors and eigen values                                            | reduces the number of variables to stabilize estimation through `regulization`                       |
+--------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+
| **Assumption**     | Data set is large enough to provide reliable estimates. There is a linear relationship between variables | Accounts for noise by assuming that data is not clearly defined                                      |
+--------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+
| **Purpose**        | when data has fewer variables(genes & lipids) compare to the samples(mice)                               | when data has a very large difference between the variables(genes and lipids) and the samples (mice) |
+--------------------+----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+

## Perform Classical Correlation

1.  number of mice (sample)

```{r}
nrow(X) == nrow(Y)
cat("The number of samples is: ",nrow(X), "\n")
```

2.  number of variables

```{r}
cat("The number of variables in genetic data is: ", ncol(X), "\n")
cat("The number of variables in lipid data is: ", ncol(Y), "\n")
```

### Subset Data to fit classical canonical analysis requirements

$$
variables\ (p) < samples\ (n)
$$

the smallest number of variables in the data sets is `21` this meets the requirement of $p<n$ so the genetic data will be subset using random values to ensure accuracy of prediction to have `21 genetic variables` too

```{r}
set.seed(222)

sub_ind = sample(seq(1, 120), 21) # random sample of 21 elements
sub_ind
```

```{r}
clasic.Genes = X[, sub_ind]
clasic.Lipid = Y
```

### Assess the correlation before classical CCA

```{r}
# gene
cl.x = as.matrix(clasic.Genes)
# lipid
cl.y = as.matrix(clasic.Genes)
# multicolinearity
multi_cor = matcor(cl.x, cl.y)


# plot the heatmap
img.matcor(multi_cor, 1)
img.matcor(multi_cor, 2)
```

> **Interpretation**
>
> The initial data is observed to have a **mostly red and orange data** (warm hues)
>
> -   The warm hues suggest a strong positive correlation between the variables of lipid and genetic data *XY correlation* $cor((x + y), (x + y))$
>
> -   The genetic data X has a positive internal correlation $cor(x, x)$
>
> -   The lipid data Y also has a positive internal correlation $cor(y, y)$
>
> -   The cross correlation $cor(x, y)$ is also positively correlated

### classical CCA

```{r}
class.result = cancor(cl.x, cl.y)

summary(class.result)
```

> **Interoretation**
>
> -   **cor** – the correlation vector generated which has `21` values
>
> -   **xcoef** – a $21 *21$ matrix of the weighted correlation covariates representing a linear combination of the genetic data
>
> -   **ycoef** – a $21 *21$ matrix of the weighted correlation covariates representing a linear combination of the lipid data
>
> -   **xcenter** and **ycenter** the total number of means used to center the genetic and lipid data

### Assess the improvement in correlation

```{r}
classCor.x = class.result$xcoef
classCor.y = class.result$ycoef
```

```{r}
# selecting the highest explained correlation to plot
plot(classCor.x[,1], classCor.y[,1], xlab="First classic canonical variable of X", ylab="First classic canonical variable of Y")

```

> **Interpretation**
>
> using the first classic canonical variable it is evident that the relationship between the genetic data and the lipid data has become strongly positively correlated
>
> This is because the resulting data point all appear in a straight line on the main diagonal of the cateasian plane.

```{r}
# multicolinearity
multiCl_cor = matcor(classCor.x, classCor.y)


# plot the heatmap
img.matcor(multiCl_cor, 1)
img.matcor(multiCl_cor, 2)
```

> **Interpretation**
>
> The classical canonical correlation aims to increase correlation across the variables in the genetic and lipid data.
>
> From the heat map for the $cor(x, y)$ the main diagonal has a completely **red color** this indicates a very strong correlation across the two variables
>
> -   the correlation within the two variables has remained positive.
>
> This satisfies the requirement that `classical CCA` does not change the correlation within the group it just makes the correlation across the two variable groups.

## Perform high-dimensional CCA

$$
p > n
$$

the variables within each group are more than the samples with the group.

the original data satisfies this requirement

### Assess the initial Distribution

```{r}
hd.x = as.matrix(X)
hd.y = as.matrix(Y)

# heat map
multi_hd = matcor(hd.x, hd.y)

img.matcor(multi_hd, 1)
img.matcor(multi_hd, 2)
```

> **Interpretation**
>
> -   $cor((x + y),(x + y))$ the heat map is mostly warm colors indicating a strong positive correlation within the two genetic and lipid data
>
> -   $cor(x, x)$ the heat map is mostly warm colors indicating a strong positive correlation within the genetic data
>
> -   $cor(y, y)$ the main diagonal is mostly red in a decreasing rate indicating a potentially positively correlated lipid data. The light blue color around indicates that most variables are not correlated within the lipid data
>
> -   $cor(x, y)$ the cross correlation indicates no correlation for the most part.However, there is a mix of strongly positively correlated data **red** and negatively correlated **blue**

### High-dimensional correlation

Using penalized multivariate analysis to perform the high dimensional correlation

```{r}
library(PMA)
```

#### Find the optimum lambda

```{r}
lambda_val = seq(0.1, 0.9, 0.1)
lambda_val
```

the optimum lambda is given when the `maximum correlation` is explained

```{r}
cor_explained = 0
optimal_lambda.1 = 0
optimal_lambda.2 = 0

# Assuming lambda_val is an array of lambda values to test
for(lambda1 in lambda_val){
    for(lambda2 in lambda_val){
        hd_result = CCA(hd.x, hd.y, K = 2,typex = "standard",typez = "standard",lambda1, lambda2) 
        if(mean(hd_result$cors) > cor_explained){
            cor_explained = mean(hd_result$cors)
            optimal_lambda.1 = lambda1 
            optimal_lambda.2 = lambda2
        }
    }
}

cat("the optimal lambda 1 is: ",optimal_lambda.1, "\n")
cat("the optimal lambda 2 is: ",optimal_lambda.2, "\n")
```

```{r}
hd_result = CCA(hd.x, hd.y, K = 21,typex = "standard",typez = "standard",0.2, 0.5) 
```

### the heat map

```{r}
hdCor_x = hd_result$u
hdCor_y = hd_result$v

# identify rows with entire 0
# non_0_x = apply(hdCor_x, 1, function(row) any(row != 0))
# non_0_y = apply(hdCor_y, 2, function(col) any(col != 0))

# transpose
hdData = data.frame(x = t(hdCor_x),
                    y = t(hdCor_y))
```

```{r}
hdData_filtered <- hdData[, apply(hdData, 2, sd) != 0]


cormat <- cor(hdData_filtered)


if (!is.null(cormat) && all(!is.na(cormat))) {
    heatmap(cormat, symm = TRUE, scale = "none")
} else {
    cat("Correlation matrix is null or contains NA values.")
}

```

> The correlation of all the variables are increased with the overall shape being red indicating a strong positive correlation

# Conclusion

when the data has more variables than sample sizes the high definition data helps remove the extreme values and allows for higher correlation for the variable groups(gene and lipid data)

the correlation is enhanced using classical CCA when the variables are less than the sample. However, there is loss of important data in the event of $p > n$ the variables being more than the sample size
